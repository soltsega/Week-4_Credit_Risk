{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1626bb6b",
   "metadata": {},
   "source": [
    "### Model Training To-Do List\n",
    "\n",
    "1. **Setup & Data Preparation**\n",
    "   - [ ] Install required packages: `mlflow`, `scikit-learn`, `pandas`, `numpy`, `matplotlib`, `seaborn`\n",
    "   - [ ] Load [final_customer_data_with_risk.csv](cci:7://file:///c:/Users/My%20Device/Desktop/Week-4_KAIM/data/processed/final_customer_data_with_risk.csv:0:0-0:0)\n",
    "   - [ ] Split data into features (X) and target (y = 'is_high_risk')\n",
    "   - [ ] Split into train/validation/test sets (80/10/10)\n",
    "\n",
    "2. **Model Training**\n",
    "   - [ ] Set up MLflow experiment tracking\n",
    "   - [ ] Train baseline models:\n",
    "     - [ ] Logistic Regression\n",
    "     - [ ] Random Forest\n",
    "     - [ ] XGBoost (optional)\n",
    "   - [ ] Log all experiments with parameters and metrics\n",
    "\n",
    "3. **Hyperparameter Tuning**\n",
    "   - [ ] Tune best performing model using GridSearchCV/RandomizedSearchCV\n",
    "   - [ ] Log best parameters and retrain model\n",
    "\n",
    "4. **Model Evaluation**\n",
    "   - [ ] Evaluate on validation set:\n",
    "     - [ ] Accuracy, Precision, Recall, F1\n",
    "     - [ ] ROC-AUC score\n",
    "     - [ ] Confusion matrix\n",
    "   - [ ] Generate feature importance plots\n",
    "\n",
    "5. **Final Model**\n",
    "   - [ ] Train final model on train+validation data\n",
    "   - [ ] Evaluate on test set\n",
    "   - [ ] Save the best model\n",
    "\n",
    "6. **Documentation**\n",
    "   - [ ] Add markdown cells explaining each step\n",
    "   - [ ] Include visualizations\n",
    "   - [ ] Document key findings and model performance\n",
    "\n",
    "7. **Cleanup**\n",
    "   - [ ] Remove any temporary code\n",
    "   - [ ] Ensure all cells run in sequence\n",
    "   - [ ] Save and commit changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7884832c",
   "metadata": {},
   "source": [
    "### 1. **Setup & Data Preparation**\n",
    "   - **Install required packages**: Install all necessary libraries for data processing, model training, and visualization.\n",
    "   - **Load the dataset**: Read [final_customer_data_with_risk.csv](cci:7://file:///c:/Users/My%20Device/Desktop/Week-4_KAIM/data/processed/final_customer_data_with_risk.csv:0:0-0:0) into a pandas DataFrame.\n",
    "   - **Split into features and target**: \n",
    "     - Features (X): All columns except `is_high_risk` (e.g., RFM metrics, transaction history).\n",
    "     - Target (y): The `is_high_risk` column (0 or 1).\n",
    "   - **Train/Validation/Test Split**:\n",
    "     - 80% for training, 10% for validation, and 10% for testing.\n",
    "     - Use `train_test_split` with `stratify=y` to maintain class distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Model Training**\n",
    "   - **Set up MLflow**: Initialize MLflow to log experiments, parameters, and metrics.\n",
    "   - **Train baseline models**:\n",
    "     - **Logistic Regression**: A simple, interpretable model to establish a baseline.\n",
    "     - **Random Forest**: Handles non-linear relationships and feature interactions.\n",
    "     - **XGBoost (optional)**: A powerful gradient-boosted tree model for better performance.\n",
    "   - **Log experiments**: Track model parameters, metrics, and artifacts (e.g., plots, feature importance) in MLflow.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Hyperparameter Tuning**\n",
    "   - **Select the best-performing model** (e.g., Random Forest).\n",
    "   - **Define a hyperparameter grid** (e.g., `n_estimators`, `max_depth`).\n",
    "   - **Use `GridSearchCV` or `RandomizedSearchCV`** to find the best hyperparameters.\n",
    "   - **Log the best parameters** and retrain the model on the full training set.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Model Evaluation**\n",
    "   - **Evaluate on the validation set**:\n",
    "     - **Metrics**: Calculate accuracy, precision, recall, F1-score, and ROC-AUC.\n",
    "     - **Confusion Matrix**: Visualize true/false positives/negatives.\n",
    "     - **ROC Curve**: Plot the trade-off between true positive rate and false positive rate.\n",
    "   - **Feature Importance**: Identify which features most influence the model's predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Final Model**\n",
    "   - **Combine training and validation sets** for the final training.\n",
    "   - **Retrain the best model** on this combined dataset.\n",
    "   - **Evaluate on the test set** to get an unbiased estimate of performance.\n",
    "   - **Save the model** (e.g., using `joblib` or `pickle`) for future use.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Documentation**\n",
    "   - **Add markdown cells** to explain each step clearly.\n",
    "   - **Include visualizations** (e.g., ROC curves, confusion matrices, feature importance plots).\n",
    "   - **Summarize findings**: Note which model performed best, key insights, and potential improvements.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Cleanup**\n",
    "   - **Remove any temporary or redundant code** to keep the notebook clean.\n",
    "   - **Ensure all cells run in sequence** without errors.\n",
    "   - **Save the notebook** and commit changes to your Git repository.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps:\n",
    "1. **Start with the first step** (Setup & Data Preparation) and run each cell to ensure everything loads correctly.\n",
    "2. **Proceed incrementally**, checking outputs at each stage.\n",
    "3. **Use MLflow** to track experiments and compare models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3ef65",
   "metadata": {},
   "source": [
    "## Checks all the dependencies and write on the requirements file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1035602d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages are already in requirements.txt\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "def get_installed_packages():\n",
    "    \"\"\"Get a set of lowercase package names that are currently installed.\"\"\"\n",
    "    if sys.version_info >= (3, 8):\n",
    "        return {pkg.metadata['Name'].lower() for pkg in importlib.metadata.distributions()}\n",
    "    else:\n",
    "        # Fallback for Python < 3.8\n",
    "        import pkg_resources\n",
    "        return {pkg.key.lower() for pkg in pkg_resources.working_set}\n",
    "\n",
    "def update_requirements(requirements_path='requirements.txt'):\n",
    "    # List of required packages\n",
    "    required_packages = [\n",
    "        'pandas',\n",
    "        'numpy',\n",
    "        'scikit-learn',\n",
    "        'matplotlib',\n",
    "        'seaborn',\n",
    "        'mlflow',\n",
    "        'xgboost',\n",
    "        'ipykernel',\n",
    "        'jupyter',\n",
    "        'scipy',\n",
    "        'imbalanced-learn',\n",
    "        'pytest',\n",
    "        'pytest-cov'\n",
    "    ]\n",
    "    \n",
    "    # Read existing requirements\n",
    "    req_file = Path(requirements_path)\n",
    "    if req_file.exists():\n",
    "        with open(req_file, 'r') as f:\n",
    "            existing_packages = {line.split('==')[0].lower().strip() for line in f if line.strip()}\n",
    "    else:\n",
    "        existing_packages = set()\n",
    "    \n",
    "    # Get installed packages\n",
    "    installed_packages = get_installed_packages()\n",
    "    \n",
    "    # Find missing packages\n",
    "    missing_packages = [pkg for pkg in required_packages \n",
    "                       if pkg.lower() not in {p.lower() for p in existing_packages} \n",
    "                       and pkg.lower() not in installed_packages]\n",
    "    \n",
    "    # Update requirements.txt if needed\n",
    "    if missing_packages:\n",
    "        print(\"Adding missing packages to requirements.txt:\")\n",
    "        with open(requirements_path, 'a') as f:\n",
    "            for pkg in missing_packages:\n",
    "                try:\n",
    "                    # Get the installed version\n",
    "                    version = importlib.metadata.version(pkg)\n",
    "                    f.write(f\"{pkg}=={version}\\n\")\n",
    "                    print(f\"✓ Added {pkg}=={version}\")\n",
    "                except importlib.metadata.PackageNotFoundError:\n",
    "                    print(f\"⚠ {pkg} not installed. Will attempt to install...\")\n",
    "    else:\n",
    "        print(\"All required packages are already in requirements.txt\")\n",
    "    \n",
    "    # Install missing packages\n",
    "    if missing_packages:\n",
    "        print(\"\\nInstalling missing packages...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + missing_packages)\n",
    "        print(\"✓ Installation complete!\")\n",
    "\n",
    "# Run the function\n",
    "update_requirements()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
