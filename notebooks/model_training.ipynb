{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1626bb6b",
   "metadata": {},
   "source": [
    "### Model Training To-Do List\n",
    "\n",
    "1. **Setup & Data Preparation**\n",
    "   - [ ] Install required packages: `mlflow`, `scikit-learn`, `pandas`, `numpy`, `matplotlib`, `seaborn`\n",
    "   - [ ] Load [final_customer_data_with_risk.csv](cci:7://file:///c:/Users/My%20Device/Desktop/Week-4_KAIM/data/processed/final_customer_data_with_risk.csv:0:0-0:0)\n",
    "   - [ ] Split data into features (X) and target (y = 'is_high_risk')\n",
    "   - [ ] Split into train/validation/test sets (80/10/10)\n",
    "\n",
    "2. **Model Training**\n",
    "   - [ ] Set up MLflow experiment tracking\n",
    "   - [ ] Train baseline models:\n",
    "     - [ ] Logistic Regression\n",
    "     - [ ] Random Forest\n",
    "     - [ ] XGBoost (optional)\n",
    "   - [ ] Log all experiments with parameters and metrics\n",
    "\n",
    "3. **Hyperparameter Tuning**\n",
    "   - [ ] Tune best performing model using GridSearchCV/RandomizedSearchCV\n",
    "   - [ ] Log best parameters and retrain model\n",
    "\n",
    "4. **Model Evaluation**\n",
    "   - [ ] Evaluate on validation set:\n",
    "     - [ ] Accuracy, Precision, Recall, F1\n",
    "     - [ ] ROC-AUC score\n",
    "     - [ ] Confusion matrix\n",
    "   - [ ] Generate feature importance plots\n",
    "\n",
    "5. **Final Model**\n",
    "   - [ ] Train final model on train+validation data\n",
    "   - [ ] Evaluate on test set\n",
    "   - [ ] Save the best model\n",
    "\n",
    "6. **Documentation**\n",
    "   - [ ] Add markdown cells explaining each step\n",
    "   - [ ] Include visualizations\n",
    "   - [ ] Document key findings and model performance\n",
    "\n",
    "7. **Cleanup**\n",
    "   - [ ] Remove any temporary code\n",
    "   - [ ] Ensure all cells run in sequence\n",
    "   - [ ] Save and commit changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7884832c",
   "metadata": {},
   "source": [
    "### 1. **Setup & Data Preparation**\n",
    "   - **Install required packages**: Install all necessary libraries for data processing, model training, and visualization.\n",
    "   - **Load the dataset**: Read [final_customer_data_with_risk.csv](cci:7://file:///c:/Users/My%20Device/Desktop/Week-4_KAIM/data/processed/final_customer_data_with_risk.csv:0:0-0:0) into a pandas DataFrame.\n",
    "   - **Split into features and target**: \n",
    "     - Features (X): All columns except `is_high_risk` (e.g., RFM metrics, transaction history).\n",
    "     - Target (y): The `is_high_risk` column (0 or 1).\n",
    "   - **Train/Validation/Test Split**:\n",
    "     - 80% for training, 10% for validation, and 10% for testing.\n",
    "     - Use `train_test_split` with `stratify=y` to maintain class distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Model Training**\n",
    "   - **Set up MLflow**: Initialize MLflow to log experiments, parameters, and metrics.\n",
    "   - **Train baseline models**:\n",
    "     - **Logistic Regression**: A simple, interpretable model to establish a baseline.\n",
    "     - **Random Forest**: Handles non-linear relationships and feature interactions.\n",
    "     - **XGBoost (optional)**: A powerful gradient-boosted tree model for better performance.\n",
    "   - **Log experiments**: Track model parameters, metrics, and artifacts (e.g., plots, feature importance) in MLflow.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Hyperparameter Tuning**\n",
    "   - **Select the best-performing model** (e.g., Random Forest).\n",
    "   - **Define a hyperparameter grid** (e.g., `n_estimators`, `max_depth`).\n",
    "   - **Use `GridSearchCV` or `RandomizedSearchCV`** to find the best hyperparameters.\n",
    "   - **Log the best parameters** and retrain the model on the full training set.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Model Evaluation**\n",
    "   - **Evaluate on the validation set**:\n",
    "     - **Metrics**: Calculate accuracy, precision, recall, F1-score, and ROC-AUC.\n",
    "     - **Confusion Matrix**: Visualize true/false positives/negatives.\n",
    "     - **ROC Curve**: Plot the trade-off between true positive rate and false positive rate.\n",
    "   - **Feature Importance**: Identify which features most influence the model's predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Final Model**\n",
    "   - **Combine training and validation sets** for the final training.\n",
    "   - **Retrain the best model** on this combined dataset.\n",
    "   - **Evaluate on the test set** to get an unbiased estimate of performance.\n",
    "   - **Save the model** (e.g., using `joblib` or `pickle`) for future use.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Documentation**\n",
    "   - **Add markdown cells** to explain each step clearly.\n",
    "   - **Include visualizations** (e.g., ROC curves, confusion matrices, feature importance plots).\n",
    "   - **Summarize findings**: Note which model performed best, key insights, and potential improvements.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Cleanup**\n",
    "   - **Remove any temporary or redundant code** to keep the notebook clean.\n",
    "   - **Ensure all cells run in sequence** without errors.\n",
    "   - **Save the notebook** and commit changes to your Git repository.\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps:\n",
    "1. **Start with the first step** (Setup & Data Preparation) and run each cell to ensure everything loads correctly.\n",
    "2. **Proceed incrementally**, checking outputs at each stage.\n",
    "3. **Use MLflow** to track experiments and compare models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb3ef65",
   "metadata": {},
   "source": [
    "## Checks all the dependencies and write on the requirements file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1035602d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required packages are already in requirements.txt\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "def get_installed_packages():\n",
    "    \"\"\"Get a set of lowercase package names that are currently installed.\"\"\"\n",
    "    if sys.version_info >= (3, 8):\n",
    "        return {pkg.metadata['Name'].lower() for pkg in importlib.metadata.distributions()}\n",
    "    else:\n",
    "        # Fallback for Python < 3.8\n",
    "        import pkg_resources\n",
    "        return {pkg.key.lower() for pkg in pkg_resources.working_set}\n",
    "\n",
    "def update_requirements(requirements_path='requirements.txt'):\n",
    "    # List of required packages\n",
    "    required_packages = [\n",
    "        'pandas',\n",
    "        'numpy',\n",
    "        'scikit-learn',\n",
    "        'matplotlib',\n",
    "        'seaborn',\n",
    "        'mlflow',\n",
    "        'xgboost',\n",
    "        'ipykernel',\n",
    "        'jupyter',\n",
    "        'scipy',\n",
    "        'imbalanced-learn',\n",
    "        'pytest',\n",
    "        'pytest-cov'\n",
    "    ]\n",
    "    \n",
    "    # Read existing requirements\n",
    "    req_file = Path(requirements_path)\n",
    "    if req_file.exists():\n",
    "        with open(req_file, 'r') as f:\n",
    "            existing_packages = {line.split('==')[0].lower().strip() for line in f if line.strip()}\n",
    "    else:\n",
    "        existing_packages = set()\n",
    "    \n",
    "    # Get installed packages\n",
    "    installed_packages = get_installed_packages()\n",
    "    \n",
    "    # Find missing packages\n",
    "    missing_packages = [pkg for pkg in required_packages \n",
    "                       if pkg.lower() not in {p.lower() for p in existing_packages} \n",
    "                       and pkg.lower() not in installed_packages]\n",
    "    \n",
    "    # Update requirements.txt if needed\n",
    "    if missing_packages:\n",
    "        print(\"Adding missing packages to requirements.txt:\")\n",
    "        with open(requirements_path, 'a') as f:\n",
    "            for pkg in missing_packages:\n",
    "                try:\n",
    "                    # Get the installed version\n",
    "                    version = importlib.metadata.version(pkg)\n",
    "                    f.write(f\"{pkg}=={version}\\n\")\n",
    "                    print(f\"✓ Added {pkg}=={version}\")\n",
    "                except importlib.metadata.PackageNotFoundError:\n",
    "                    print(f\"⚠ {pkg} not installed. Will attempt to install...\")\n",
    "    else:\n",
    "        print(\"All required packages are already in requirements.txt\")\n",
    "    \n",
    "    # Install missing packages\n",
    "    if missing_packages:\n",
    "        print(\"\\nInstalling missing packages...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + missing_packages)\n",
    "        print(\"✓ Installation complete!\")\n",
    "\n",
    "# Run the function\n",
    "update_requirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316204c",
   "metadata": {},
   "source": [
    "## Import the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9da917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222db30",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3b37418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (95662, 18)\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionId</th>\n",
       "      <th>BatchId</th>\n",
       "      <th>AccountId</th>\n",
       "      <th>SubscriptionId</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>CurrencyCode</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>ProviderId</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>ProductCategory</th>\n",
       "      <th>ChannelId</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Value</th>\n",
       "      <th>TransactionStartTime</th>\n",
       "      <th>PricingStrategy</th>\n",
       "      <th>FraudResult</th>\n",
       "      <th>Risk_Label</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TransactionId_76871</td>\n",
       "      <td>BatchId_36123</td>\n",
       "      <td>AccountId_3957</td>\n",
       "      <td>SubscriptionId_887</td>\n",
       "      <td>CustomerId_4406</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_6</td>\n",
       "      <td>ProductId_10</td>\n",
       "      <td>airtime</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>2018-11-15T02:18:49Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Medium Risk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TransactionId_73770</td>\n",
       "      <td>BatchId_15642</td>\n",
       "      <td>AccountId_4841</td>\n",
       "      <td>SubscriptionId_3829</td>\n",
       "      <td>CustomerId_4406</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_4</td>\n",
       "      <td>ProductId_6</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>ChannelId_2</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2018-11-15T02:19:08Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Medium Risk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TransactionId_26203</td>\n",
       "      <td>BatchId_53941</td>\n",
       "      <td>AccountId_4229</td>\n",
       "      <td>SubscriptionId_222</td>\n",
       "      <td>CustomerId_4683</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_6</td>\n",
       "      <td>ProductId_1</td>\n",
       "      <td>airtime</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500</td>\n",
       "      <td>2018-11-15T02:44:21Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TransactionId_380</td>\n",
       "      <td>BatchId_102363</td>\n",
       "      <td>AccountId_648</td>\n",
       "      <td>SubscriptionId_2185</td>\n",
       "      <td>CustomerId_988</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_1</td>\n",
       "      <td>ProductId_21</td>\n",
       "      <td>utility_bill</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>21800</td>\n",
       "      <td>2018-11-15T03:32:55Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Medium Risk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TransactionId_28195</td>\n",
       "      <td>BatchId_38780</td>\n",
       "      <td>AccountId_4841</td>\n",
       "      <td>SubscriptionId_3829</td>\n",
       "      <td>CustomerId_988</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_4</td>\n",
       "      <td>ProductId_6</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>ChannelId_2</td>\n",
       "      <td>-644.0</td>\n",
       "      <td>644</td>\n",
       "      <td>2018-11-15T03:34:21Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Medium Risk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TransactionId         BatchId       AccountId       SubscriptionId  \\\n",
       "0  TransactionId_76871   BatchId_36123  AccountId_3957   SubscriptionId_887   \n",
       "1  TransactionId_73770   BatchId_15642  AccountId_4841  SubscriptionId_3829   \n",
       "2  TransactionId_26203   BatchId_53941  AccountId_4229   SubscriptionId_222   \n",
       "3    TransactionId_380  BatchId_102363   AccountId_648  SubscriptionId_2185   \n",
       "4  TransactionId_28195   BatchId_38780  AccountId_4841  SubscriptionId_3829   \n",
       "\n",
       "        CustomerId CurrencyCode  CountryCode    ProviderId     ProductId  \\\n",
       "0  CustomerId_4406          UGX          256  ProviderId_6  ProductId_10   \n",
       "1  CustomerId_4406          UGX          256  ProviderId_4   ProductId_6   \n",
       "2  CustomerId_4683          UGX          256  ProviderId_6   ProductId_1   \n",
       "3   CustomerId_988          UGX          256  ProviderId_1  ProductId_21   \n",
       "4   CustomerId_988          UGX          256  ProviderId_4   ProductId_6   \n",
       "\n",
       "      ProductCategory    ChannelId   Amount  Value  TransactionStartTime  \\\n",
       "0             airtime  ChannelId_3   1000.0   1000  2018-11-15T02:18:49Z   \n",
       "1  financial_services  ChannelId_2    -20.0     20  2018-11-15T02:19:08Z   \n",
       "2             airtime  ChannelId_3    500.0    500  2018-11-15T02:44:21Z   \n",
       "3        utility_bill  ChannelId_3  20000.0  21800  2018-11-15T03:32:55Z   \n",
       "4  financial_services  ChannelId_2   -644.0    644  2018-11-15T03:34:21Z   \n",
       "\n",
       "   PricingStrategy  FraudResult   Risk_Label  Cluster  \n",
       "0                2            0  Medium Risk        1  \n",
       "1                2            0  Medium Risk        1  \n",
       "2                2            0     Low Risk        0  \n",
       "3                2            0  Medium Risk        1  \n",
       "4                2            0  Medium Risk        1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the processed data\n",
    "data_path = '../data/processed/final_customer_data_with_risk.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic info\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0af011c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in the DataFrame:\n",
      "['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'CurrencyCode', 'CountryCode', 'ProviderId', 'ProductId', 'ProductCategory', 'ChannelId', 'Amount', 'Value', 'TransactionStartTime', 'PricingStrategy', 'FraudResult', 'Risk_Label', 'Cluster']\n",
      "\n",
      "First 5 rows of the DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionId</th>\n",
       "      <th>BatchId</th>\n",
       "      <th>AccountId</th>\n",
       "      <th>SubscriptionId</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>CurrencyCode</th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>ProviderId</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>ProductCategory</th>\n",
       "      <th>ChannelId</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Value</th>\n",
       "      <th>TransactionStartTime</th>\n",
       "      <th>PricingStrategy</th>\n",
       "      <th>FraudResult</th>\n",
       "      <th>Risk_Label</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TransactionId_76871</td>\n",
       "      <td>BatchId_36123</td>\n",
       "      <td>AccountId_3957</td>\n",
       "      <td>SubscriptionId_887</td>\n",
       "      <td>CustomerId_4406</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_6</td>\n",
       "      <td>ProductId_10</td>\n",
       "      <td>airtime</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>2018-11-15T02:18:49Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Medium Risk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TransactionId_73770</td>\n",
       "      <td>BatchId_15642</td>\n",
       "      <td>AccountId_4841</td>\n",
       "      <td>SubscriptionId_3829</td>\n",
       "      <td>CustomerId_4406</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_4</td>\n",
       "      <td>ProductId_6</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>ChannelId_2</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>20</td>\n",
       "      <td>2018-11-15T02:19:08Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Medium Risk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TransactionId_26203</td>\n",
       "      <td>BatchId_53941</td>\n",
       "      <td>AccountId_4229</td>\n",
       "      <td>SubscriptionId_222</td>\n",
       "      <td>CustomerId_4683</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_6</td>\n",
       "      <td>ProductId_1</td>\n",
       "      <td>airtime</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>500.0</td>\n",
       "      <td>500</td>\n",
       "      <td>2018-11-15T02:44:21Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Low Risk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TransactionId_380</td>\n",
       "      <td>BatchId_102363</td>\n",
       "      <td>AccountId_648</td>\n",
       "      <td>SubscriptionId_2185</td>\n",
       "      <td>CustomerId_988</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_1</td>\n",
       "      <td>ProductId_21</td>\n",
       "      <td>utility_bill</td>\n",
       "      <td>ChannelId_3</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>21800</td>\n",
       "      <td>2018-11-15T03:32:55Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Medium Risk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TransactionId_28195</td>\n",
       "      <td>BatchId_38780</td>\n",
       "      <td>AccountId_4841</td>\n",
       "      <td>SubscriptionId_3829</td>\n",
       "      <td>CustomerId_988</td>\n",
       "      <td>UGX</td>\n",
       "      <td>256</td>\n",
       "      <td>ProviderId_4</td>\n",
       "      <td>ProductId_6</td>\n",
       "      <td>financial_services</td>\n",
       "      <td>ChannelId_2</td>\n",
       "      <td>-644.0</td>\n",
       "      <td>644</td>\n",
       "      <td>2018-11-15T03:34:21Z</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Medium Risk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TransactionId         BatchId       AccountId       SubscriptionId  \\\n",
       "0  TransactionId_76871   BatchId_36123  AccountId_3957   SubscriptionId_887   \n",
       "1  TransactionId_73770   BatchId_15642  AccountId_4841  SubscriptionId_3829   \n",
       "2  TransactionId_26203   BatchId_53941  AccountId_4229   SubscriptionId_222   \n",
       "3    TransactionId_380  BatchId_102363   AccountId_648  SubscriptionId_2185   \n",
       "4  TransactionId_28195   BatchId_38780  AccountId_4841  SubscriptionId_3829   \n",
       "\n",
       "        CustomerId CurrencyCode  CountryCode    ProviderId     ProductId  \\\n",
       "0  CustomerId_4406          UGX          256  ProviderId_6  ProductId_10   \n",
       "1  CustomerId_4406          UGX          256  ProviderId_4   ProductId_6   \n",
       "2  CustomerId_4683          UGX          256  ProviderId_6   ProductId_1   \n",
       "3   CustomerId_988          UGX          256  ProviderId_1  ProductId_21   \n",
       "4   CustomerId_988          UGX          256  ProviderId_4   ProductId_6   \n",
       "\n",
       "      ProductCategory    ChannelId   Amount  Value  TransactionStartTime  \\\n",
       "0             airtime  ChannelId_3   1000.0   1000  2018-11-15T02:18:49Z   \n",
       "1  financial_services  ChannelId_2    -20.0     20  2018-11-15T02:19:08Z   \n",
       "2             airtime  ChannelId_3    500.0    500  2018-11-15T02:44:21Z   \n",
       "3        utility_bill  ChannelId_3  20000.0  21800  2018-11-15T03:32:55Z   \n",
       "4  financial_services  ChannelId_2   -644.0    644  2018-11-15T03:34:21Z   \n",
       "\n",
       "   PricingStrategy  FraudResult   Risk_Label  Cluster  \n",
       "0                2            0  Medium Risk        1  \n",
       "1                2            0  Medium Risk        1  \n",
       "2                2            0     Low Risk        0  \n",
       "3                2            0  Medium Risk        1  \n",
       "4                2            0  Medium Risk        1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display all column names\n",
    "print(\"Available columns in the DataFrame:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display the first few rows to see the data\n",
    "print(\"\\nFirst 5 rows of the DataFrame:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1764fafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "TransactionId           0\n",
      "BatchId                 0\n",
      "AccountId               0\n",
      "SubscriptionId          0\n",
      "CustomerId              0\n",
      "CurrencyCode            0\n",
      "CountryCode             0\n",
      "ProviderId              0\n",
      "ProductId               0\n",
      "ProductCategory         0\n",
      "ChannelId               0\n",
      "Amount                  0\n",
      "Value                   0\n",
      "TransactionStartTime    0\n",
      "PricingStrategy         0\n",
      "FraudResult             0\n",
      "Risk_Label              0\n",
      "Cluster                 0\n",
      "dtype: int64\n",
      "\n",
      "Class distribution:\n",
      "Risk_Label\n",
      "Medium Risk    0.761661\n",
      "High Risk      0.208975\n",
      "Low Risk       0.029364\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Numerical features statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CountryCode</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Value</th>\n",
       "      <th>PricingStrategy</th>\n",
       "      <th>FraudResult</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>95662.0</td>\n",
       "      <td>9.566200e+04</td>\n",
       "      <td>9.566200e+04</td>\n",
       "      <td>95662.000000</td>\n",
       "      <td>95662.000000</td>\n",
       "      <td>95662.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>256.0</td>\n",
       "      <td>6.717846e+03</td>\n",
       "      <td>9.900584e+03</td>\n",
       "      <td>2.255974</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>1.179612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.233068e+05</td>\n",
       "      <td>1.231221e+05</td>\n",
       "      <td>0.732924</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.453961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>256.0</td>\n",
       "      <td>-1.000000e+06</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>256.0</td>\n",
       "      <td>-5.000000e+01</td>\n",
       "      <td>2.750000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>256.0</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>256.0</td>\n",
       "      <td>2.800000e+03</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>256.0</td>\n",
       "      <td>9.880000e+06</td>\n",
       "      <td>9.880000e+06</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CountryCode        Amount         Value  PricingStrategy   FraudResult  \\\n",
       "count      95662.0  9.566200e+04  9.566200e+04     95662.000000  95662.000000   \n",
       "mean         256.0  6.717846e+03  9.900584e+03         2.255974      0.002018   \n",
       "std            0.0  1.233068e+05  1.231221e+05         0.732924      0.044872   \n",
       "min          256.0 -1.000000e+06  2.000000e+00         0.000000      0.000000   \n",
       "25%          256.0 -5.000000e+01  2.750000e+02         2.000000      0.000000   \n",
       "50%          256.0  1.000000e+03  1.000000e+03         2.000000      0.000000   \n",
       "75%          256.0  2.800000e+03  5.000000e+03         2.000000      0.000000   \n",
       "max          256.0  9.880000e+06  9.880000e+06         4.000000      1.000000   \n",
       "\n",
       "            Cluster  \n",
       "count  95662.000000  \n",
       "mean       1.179612  \n",
       "std        0.453961  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        2.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "y = df['Risk_Label']  # Changed from 'is_high_risk' to 'Risk_Label'\n",
    "X = df.drop('Risk_Label', axis=1)  # Update this line as well\n",
    "\n",
    "#distribution check\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['Risk_Label'].value_counts(normalize=True))\n",
    "# Basic statistics\n",
    "print(\"\\nNumerical features statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604961cf",
   "metadata": {},
   "source": [
    "## Train/Test/validate split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42fcc16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (38264, 17) (38264,)\n",
      "Validation set: (38265, 17) (38265,)\n",
      "Test set: (19133, 17) (19133,)\n",
      "\n",
      "Class distribution in training set:\n",
      "Risk_Label\n",
      "Medium Risk    0.761656\n",
      "High Risk      0.208969\n",
      "Low Risk       0.029375\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in validation set:\n",
      "Risk_Label\n",
      "Medium Risk    0.761662\n",
      "High Risk      0.208990\n",
      "Low Risk       0.029348\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in test set:\n",
      "Risk_Label\n",
      "Medium Risk    0.761668\n",
      "High Risk      0.208958\n",
      "Low Risk       0.029373\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: 80% training, 20% temp\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Split temp into validation and test (50/50 of temp = 10% each of total)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Print the shapes\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Check class distribution in each set\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in validation set:\")\n",
    "print(y_val.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc0091a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Original data shape: (95662, 18)\n",
      "\n",
      "Saving split datasets...\n",
      "\n",
      "Dataset sizes:\n",
      "Training set: 66,963 samples\n",
      "Validation set: 14,349 samples\n",
      "Test set: 14,350 samples\n",
      "\n",
      "Class distribution:\n",
      "\n",
      "Training set:\n",
      "Risk_Label\n",
      "High Risk      0.208981\n",
      "Low Risk       0.029359\n",
      "Medium Risk    0.761659\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Validation set:\n",
      "Risk_Label\n",
      "High Risk      0.208934\n",
      "Low Risk       0.029410\n",
      "Medium Risk    0.761656\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set:\n",
      "Risk_Label\n",
      "High Risk      0.208990\n",
      "Low Risk       0.029338\n",
      "Medium Risk    0.761672\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "All datasets saved to: C:\\Users\\My Device\\Desktop\\Week-4_KAIM\\data\\splits\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# File paths\n",
    "input_file = r\"C:\\Users\\My Device\\Desktop\\Week-4_KAIM\\data\\processed\\final_customer_data_with_risk.csv\"\n",
    "output_dir = r\"C:\\Users\\My Device\\Desktop\\Week-4_KAIM\\data\\splits\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def load_and_split_data():\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Original data shape: {df.shape}\")\n",
    "    \n",
    "    # Check if target column exists\n",
    "    target_col = 'Risk_Label'  # Change this if your target column has a different name\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in the data\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # First split: 70% training, 30% temp\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.3, \n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Split temp into validation and test (50/50 of temp = 15% each of total)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=0.5,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Add target back to features for saving\n",
    "    train_data = X_train.copy()\n",
    "    train_data[target_col] = y_train\n",
    "    \n",
    "    val_data = X_val.copy()\n",
    "    val_data[target_col] = y_val\n",
    "    \n",
    "    test_data = X_test.copy()\n",
    "    test_data[target_col] = y_test\n",
    "    \n",
    "    # Save the splits\n",
    "    print(\"\\nSaving split datasets...\")\n",
    "    train_data.to_csv(os.path.join(output_dir, \"train.csv\"), index=False)\n",
    "    val_data.to_csv(os.path.join(output_dir, \"val.csv\"), index=False)\n",
    "    test_data.to_csv(os.path.join(output_dir, \"test.csv\"), index=False)\n",
    "    \n",
    "    # Print dataset sizes\n",
    "    print(\"\\nDataset sizes:\")\n",
    "    print(f\"Training set: {len(train_data):,} samples\")\n",
    "    print(f\"Validation set: {len(val_data):,} samples\")\n",
    "    print(f\"Test set: {len(test_data):,} samples\")\n",
    "    \n",
    "    # Print class distribution\n",
    "    print(\"\\nClass distribution:\")\n",
    "    for name, data in [('Training', train_data), ('Validation', val_data), ('Test', test_data)]:\n",
    "        print(f\"\\n{name} set:\")\n",
    "        print(data[target_col].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    print(f\"\\nAll datasets saved to: {output_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_and_split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471162fa",
   "metadata": {},
   "source": [
    "## Load the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab592287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Training set: 66963 samples\n",
      "Validation set: 14349 samples\n",
      "Test set: 14350 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the split data\n",
    "data_dir = Path('../data/splits/')\n",
    "X_train = pd.read_csv(data_dir / 'train.csv')\n",
    "X_val = pd.read_csv(data_dir / 'val.csv')\n",
    "X_test = pd.read_csv(data_dir / 'test.csv')\n",
    "\n",
    "# The target column is 'Risk_Label' in each file\n",
    "y_train = X_train.pop('Risk_Label')\n",
    "y_val = X_val.pop('Risk_Label')\n",
    "y_test = X_test.pop('Risk_Label')\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f1cafa",
   "metadata": {},
   "source": [
    "## Encode the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa656e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Training set: 66963 samples\n",
      "Validation set: 14349 samples\n",
      "Test set: 14350 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the split data\n",
    "data_dir = Path('../data/splits/')\n",
    "X_train = pd.read_csv(data_dir / 'train.csv')\n",
    "X_val = pd.read_csv(data_dir / 'val.csv')\n",
    "X_test = pd.read_csv(data_dir / 'test.csv')\n",
    "\n",
    "# The target column is 'Risk_Label' in each file\n",
    "y_train = X_train.pop('Risk_Label')\n",
    "y_val = X_val.pop('Risk_Label')\n",
    "y_test = X_test.pop('Risk_Label')\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd7f401",
   "metadata": {},
   "source": [
    "## Set up the mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6caa724e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location=('file:c:/Users/My '\n",
       " 'Device/Desktop/Week-4_KAIM/notebooks/mlruns/675144119365642279'), creation_time=1765893232336, experiment_id='675144119365642279', last_update_time=1765893232336, lifecycle_stage='active', name='Credit_Risk_Prediction', tags={}>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Set up MLflow\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"Credit_Risk_Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed429a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (66963, 17)\n",
      "Validation set shape: (14349, 17)\n",
      "Test set shape: (14350, 17)\n",
      "\n",
      "Class distribution in training set:\n",
      "Risk_Label\n",
      "Medium Risk    0.761659\n",
      "High Risk      0.208981\n",
      "Low Risk       0.029359\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d4fb0b",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b14a99ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'CurrencyCode', 'ProviderId', 'ProductId', 'ProductCategory', 'ChannelId', 'TransactionStartTime']\n",
      "Numeric columns: ['CountryCode', 'Amount', 'Value', 'PricingStrategy', 'FraudResult', 'Cluster']\n",
      "\n",
      "Fitting and transforming data...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 105. GiB for an array with shape (66963, 210020) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Fit and transform the data\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFitting and transforming data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m X_train_processed = \u001b[43mpreprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m X_val_processed = preprocessor.transform(X_val)\n\u001b[32m     35\u001b[39m X_test_processed = preprocessor.transform(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:999\u001b[39m, in \u001b[36mColumnTransformer.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    997\u001b[39m     routed_params = \u001b[38;5;28mself\u001b[39m._get_empty_routing()\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_func_on_transformers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_fit_transform_one\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_as_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[32m   1008\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_fitted_transformers([])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:901\u001b[39m, in \u001b[36mColumnTransformer._call_func_on_transformers\u001b[39m\u001b[34m(self, X, y, func, column_as_labels, routed_params)\u001b[39m\n\u001b[32m    889\u001b[39m             extra_args = {}\n\u001b[32m    890\u001b[39m         jobs.append(\n\u001b[32m    891\u001b[39m             delayed(func)(\n\u001b[32m    892\u001b[39m                 transformer=clone(trans) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted \u001b[38;5;28;01melse\u001b[39;00m trans,\n\u001b[32m   (...)\u001b[39m\u001b[32m    898\u001b[39m             )\n\u001b[32m    899\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    904\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mExpected 2D array, got 1D array instead\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:91\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     79\u001b[39m warning_filters = (\n\u001b[32m     80\u001b[39m     filters_func() \u001b[38;5;28;01mif\u001b[39;00m filters_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m warnings.filters\n\u001b[32m     81\u001b[39m )\n\u001b[32m     83\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     84\u001b[39m     (\n\u001b[32m     85\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     90\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\joblib\\parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\joblib\\parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:184\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    180\u001b[39m                 this_warning_filter_dict[special_key] = this_value.pattern\n\u001b[32m    182\u001b[39m         warnings.filterwarnings(**this_warning_filter_dict, append=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\sklearn\\pipeline.py:1484\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1482\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1483\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1486\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1487\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1488\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\sklearn\\pipeline.py:689\u001b[39m, in \u001b[36mPipeline.fit_transform\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    683\u001b[39m last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    684\u001b[39m     step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    685\u001b[39m     step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    686\u001b[39m     all_params=params,\n\u001b[32m    687\u001b[39m )\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(last_step, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlast_step\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    693\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m last_step.fit(Xt, y, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m]).transform(\n\u001b[32m    694\u001b[39m         Xt, **last_step_params[\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    695\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\sklearn\\base.py:907\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    892\u001b[39m         warnings.warn(\n\u001b[32m    893\u001b[39m             (\n\u001b[32m    894\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    902\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    903\u001b[39m         )\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    906\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m907\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    909\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    910\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1091\u001b[39m, in \u001b[36mOneHotEncoder.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1085\u001b[39m out = sparse.csr_matrix(\n\u001b[32m   1086\u001b[39m     (data, indices, indptr),\n\u001b[32m   1087\u001b[39m     shape=(n_samples, feature_indices[-\u001b[32m1\u001b[39m]),\n\u001b[32m   1088\u001b[39m     dtype=\u001b[38;5;28mself\u001b[39m.dtype,\n\u001b[32m   1089\u001b[39m )\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sparse_output:\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1093\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:996\u001b[39m, in \u001b[36m_cs_matrix.toarray\u001b[39m\u001b[34m(self, order, out)\u001b[39m\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    995\u001b[39m     order = \u001b[38;5;28mself\u001b[39m._swap(\u001b[33m'\u001b[39m\u001b[33mcf\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out.flags.c_contiguous \u001b[38;5;129;01mor\u001b[39;00m out.flags.f_contiguous):\n\u001b[32m    998\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mOutput array must be C or F contiguous\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\My Device\\Desktop\\Week-4_KAIM\\.my_env\\Lib\\site-packages\\scipy\\sparse\\_base.py:1530\u001b[39m, in \u001b[36m_spbase._process_toarray_args\u001b[39m\u001b[34m(self, order, out)\u001b[39m\n\u001b[32m   1528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m   1529\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 105. GiB for an array with shape (66963, 210020) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Identify column types\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "# Create transformers\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Fit and transform the data\n",
    "print(\"\\nFitting and transforming data...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"\\nProcessed data shapes:\")\n",
    "print(\"Training:\", X_train_processed.shape)\n",
    "print(\"Validation:\", X_val_processed.shape)\n",
    "print(\"Test:\", X_test_processed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e60aef",
   "metadata": {},
   "source": [
    "## Encode the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ae591d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping: {'High Risk': np.int64(0), 'Low Risk': np.int64(1), 'Medium Risk': np.int64(2)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_val_encoded = le.transform(y_val)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "# Print class mapping\n",
    "print(\"Class mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8010c8",
   "metadata": {},
   "source": [
    "## Train a baseline model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ab95d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining Logistic Regression...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m model = LogisticRegression(max_iter=\u001b[32m1000\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model.fit(\u001b[43mX_train_processed\u001b[49m, y_train_encoded)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[32m     10\u001b[39m y_val_pred = model.predict(X_val_processed)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_processed' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Logistic Regression...\")\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model.fit(X_train_processed, y_train_encoded)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred = model.predict(X_val_processed)\n",
    "print(\"\\nValidation Results:\")\n",
    "print(classification_report(y_val_encoded, y_val_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4f7c3",
   "metadata": {},
   "source": [
    "## Train and evaluate multiple models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dc64149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic_Regression...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     model.fit(\u001b[43mX_train_processed\u001b[49m, y_train_encoded)\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Predict on validation set\u001b[39;00m\n\u001b[32m     17\u001b[39m     y_pred = model.predict(X_val_processed)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_processed' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "models = {\n",
    "    \"Logistic_Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random_Forest\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    \"Gradient_Boosting\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train_processed, y_train_encoded)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_pred = model.predict(X_val_processed)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy_score(y_val_encoded, y_pred),\n",
    "        'precision': precision_score(y_val_encoded, y_pred, average='weighted'),\n",
    "        'recall': recall_score(y_val_encoded, y_pred, average='weighted'),\n",
    "        'f1': f1_score(y_val_encoded, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(classification_report(y_val_encoded, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Compare models\n",
    "print(\"\\nModel Comparison (Weighted F1 Score):\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}: {metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfe7b4",
   "metadata": {},
   "source": [
    "##  Hyperparameter Tuning (Example with Random Forest):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35971870",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      4\u001b[39m param_grid = {\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m50\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m200\u001b[39m],\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m'\u001b[39m: [\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m10\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m30\u001b[39m],\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmin_samples_split\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m2\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m10\u001b[39m]\n\u001b[32m      8\u001b[39m }\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Initialize grid search\u001b[39;00m\n\u001b[32m     11\u001b[39m grid_search = GridSearchCV(\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mRandomForestClassifier\u001b[49m(random_state=\u001b[32m42\u001b[39m, n_jobs=-\u001b[32m1\u001b[39m),\n\u001b[32m     13\u001b[39m     param_grid,\n\u001b[32m     14\u001b[39m     cv=\u001b[32m5\u001b[39m,\n\u001b[32m     15\u001b[39m     scoring=\u001b[33m'\u001b[39m\u001b[33mf1_weighted\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     16\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m,\n\u001b[32m     17\u001b[39m     verbose=\u001b[32m1\u001b[39m\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPerforming grid search...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'RandomForestClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize grid search\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Performing grid search...\")\n",
    "grid_search.fit(X_train_processed, y_train_encoded)\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "y_val_pred = best_model.predict(X_val_processed)\n",
    "print(\"\\nBest Model Validation Results:\")\n",
    "print(classification_report(y_val_encoded, y_val_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dab88a",
   "metadata": {},
   "source": [
    "## Evaluate the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba27216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "y_test_pred = best_model.predict(X_test_processed)\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(classification_report(y_test_encoded, y_test_pred, target_names=le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
